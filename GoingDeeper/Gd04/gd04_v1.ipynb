{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트: 한영 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 14:46:18.733883: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 다운로드할 파일의 URL\n",
    "# url = \"https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-news-v1/korean-english-park.train.tar.gz\"\n",
    "\n",
    "# # 파일을 저장할 경로\n",
    "# filename = \"korean-english-park.train.tar.gz\"\n",
    "\n",
    "# # 파일 다운로드\n",
    "# response = requests.get(url)\n",
    "# with open(filename, 'wb') as file:\n",
    "#     file.write(response.content)\n",
    "\n",
    "# print(f\"{filename} 파일이 성공적으로 다운로드되었습니다.\")\n",
    "\n",
    "# # 파일 경로\n",
    "# file_name = \"korean-english-park.train.tar.gz\"\n",
    "\n",
    "# # 압축 해제 및 추출\n",
    "# with tarfile.open(file_name, \"r:gz\") as tar:\n",
    "#     tar.extractall()  # 현재 디렉토리에 파일을 추출\n",
    "#     print(f\"{file_name} 파일이 성공적으로 추출되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 데이터 개수: 94123\n",
      "영어 데이터 개수: 94123\n"
     ]
    }
   ],
   "source": [
    "kor_path = 'data/macine_translate/korean-english-park.train.ko'\n",
    "eng_path = 'data/macine_translate/korean-english-park.train.en'\n",
    "\n",
    "with open(kor_path, 'r') as f:\n",
    "    kor = f.read().splitlines()\n",
    "    \n",
    "with open(eng_path, 'r') as f:\n",
    "    eng = f.read().splitlines()\n",
    "    \n",
    "print(f\"한국어 데이터 개수: {len(kor)}\")\n",
    "print(f\"영어 데이터 개수: {len(eng)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set 데이터형이 중복을 허용하지 않는다는 것을 활용해 중복된 데이터를 제거하도록 합니다. 데이터의 병렬 쌍이 흐트러지지 않게 주의하세요! 중복을 제거한 데이터를 cleaned_corpus 에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문장 쌍의 개수: 78968\n",
      "한국어: 캄포트 지역의 누온 사리 경찰 차장은 시엠 레아프로부터 남쪽으로 비행하던 러시아제 AN-24 항공기가 산에 충돌하며 생존자가 남지 않았다고 언급했다고, AFP 통신이 보도했다.\n",
      "영어: Nuon Sary, deputy police chief of Kampot province, said that the Russian-made AN-24 flying from Siem Reap to the south seemed to have hit the mountain, leaving no survivors, according to AFP.\n"
     ]
    }
   ],
   "source": [
    "# set()을 사용하여 중복을 제거한 뒤 다시 리스트로 변환\n",
    "\n",
    "cleaned_corpus = list(set(zip(kor, eng)))\n",
    "\n",
    "print(f\"전체 문장 쌍의 개수: {len(cleaned_corpus)}\")\n",
    "\n",
    "# 한국어 list\n",
    "cleaned_kor = []\n",
    "# 영어 list\n",
    "cleaned_eng = []\n",
    "\n",
    "for kor, eng in cleaned_corpus:\n",
    "    cleaned_kor.append(kor)\n",
    "    cleaned_eng.append(eng)\n",
    "    \n",
    "print(f\"한국어: {cleaned_kor[100]}\")\n",
    "print(f\"영어: {cleaned_eng[100]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 자리에서 영화는 20분만 상영됐지만 반응이 좋아 올해 여름 시장에서 큰 호응이 예상된다.\n",
      "From the 20 minutes of footage they showed, the film looks likely to meet expectations as one of summer's hottest tickets.\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_kor[10000])\n",
    "print(cleaned_eng[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "무게가 더 나가면 연료도 많이 소비된다.\n",
      "More weight means more fuel.\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_kor[9709])\n",
    "print(cleaned_eng[9709])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이 일은 계속됩니다.', '영국 당국이 이 개정안에따라 기소 전 테러 용의자를 6주간 구금할 수 있게 된다.', '“우리는 백 년에 한번 일어날만한 신용 쓰나미의 한 가운데에 있습니다.”', '미얀마 정부는 통금령을 완화해 13일부터 밤 11시부터 새벽 3시까지로 4시간으로 줄었다.', '토지 가격이 신축 아파트 가격의 70%에 이르기 때문에, 건축 비용만 부담하게 한다면 집값을 내리는 데 도움이 될 것이지만, 정책의 실행 가능성은 아직 알 수 없다.']\n",
      "['This work continues.', 'Britain¡¯s House of Commons on Wednesday narrowly approved a counterterrorism bill that allows authorities to hold terrorism suspects without charge for up to six weeks.', '“We are in the midst of a once-in-a-century credit tsunami.', 'Authorities in the country relaxed a nighttime curfew to cover four hours from 11 p.m. to 3 a.m starting Saturday night.', 'As land costs account for up to 70 percent of new apartment prices, having people pay just the construction costs would help cut home prices, although the feasibility of the policy remains to be determined.']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_kor[1000:1005])\n",
    "print(cleaned_eng[1000:1005])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의한 preprocessing() 함수는 한글에서는 동작하지 않습니다. 한글에 적용할 수 있는 정규식을 추가하여 함수를 재정의하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess() 함수를 정의\n",
    "\n",
    "def kor_prerocess_sentence(sentence):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣?.!,]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def eng_prerocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어: 샤론 총리는 파리 방문을 계획 중이었으나 정해진 일정은 없었다 .\n",
      "영어: <start> Sharon was considering a trip to Paris , but no date had been set . <end>\n",
      "15931\n",
      "15931\n"
     ]
    }
   ],
   "source": [
    "# preprocess() 함수를 사용하여 데이터를 정제\n",
    "\n",
    "kor_corpus = []\n",
    "eng_corpus = []\n",
    "\n",
    "for kor, eng in zip(cleaned_kor, cleaned_eng):\n",
    "    # 토큰 길이가 40이하인 데이터 대상으로 정제\n",
    "    if len(kor) <= 40:\n",
    "        kor_corpus.append(kor_prerocess_sentence(kor))\n",
    "        eng_corpus.append(eng_prerocess_sentence(eng, s_token=True, e_token=True))\n",
    "    \n",
    "print(f\"한국어: {kor_corpus[100]}\")\n",
    "print(f\"영어: {eng_corpus[100]}\")\n",
    "\n",
    "print(len(kor_corpus))\n",
    "print(len(eng_corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의한 tokenize() 함수를 사용해 데이터를 텐서로 변환하고 각각의 tokenizer를 얻으세요! 단어의 수는 실험을 통해 적당한 값을 맞춰주도록 합니다! (최소 10,000 이상!)\n",
    "\n",
    "❗ 주의: 난이도에 비해 데이터가 많지 않아 훈련 데이터와 검증 데이터를 따로 나누지는 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kor_tokenizer(corpus):\n",
    "    mecab = Mecab()\n",
    "    # 형태소 분석기를 활용한 토큰화\n",
    "    tokenized_corpus = [mecab.morphs(sentence) for sentence in corpus]\n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(tokenized_corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(tokenized_corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, tokenizer\n",
    "\n",
    "def eng_tokenizer(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15931, 25)\n",
      "(15931, 71)\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "kor_tensor, kor_tokenizer = kor_tokenizer(kor_corpus)\n",
    "eng_tensor, eng_tokenizer = eng_tokenizer(eng_corpus)\n",
    "\n",
    "print(kor_tensor.shape)\n",
    "print(eng_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15545\n",
      "16834\n"
     ]
    }
   ],
   "source": [
    "print(len(kor_tokenizer.index_word))\n",
    "print(len(eng_tokenizer.index_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. 모델 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어를 영어로 잘 번역해 줄 멋진 Attention 기반 Seq2seq 모델을 설계하세요! 앞서 만든 모델에 Dropout 모듈을 추가하면 성능이 더 좋아집니다! Embedding Size와 Hidden Size는 실험을 통해 적당한 값을 맞춰 주도록 합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention - BahdanauAttention\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, dropout_rate=0.5):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "        attn = self.dropout(attn) # dropout\n",
    "        \n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, dropout_rate=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.dropout(out) # dropout\n",
    "        out = self.gru(out)\n",
    "        out = self.dropout(out) # dropout\n",
    "         \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, dropout_rate=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        \"\"\"\n",
    "        x: Decoder의 input token [batch x 1]\n",
    "        h_dec: Decoder의 이전 hidden state [batch x units]\n",
    "        enc_out: Encoder의 output sequence [batch x length x units]\n",
    "        \"\"\"\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = self.dropout(out)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = self.dropout(out) # dropout\n",
    "        out = tf.reshape(out, (-1, out.shape[2])) # [batch x length x units] -> [batch x units]\n",
    "        out = self.fc(out)\n",
    "        out = self.dropout(out) # dropout\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 12:38:52.660429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9621 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:19:00.0, compute capability: 7.5\n",
      "2024-12-20 12:38:52.660990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9621 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "2024-12-20 12:38:52.661532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9621 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:67:00.0, compute capability: 7.5\n",
      "2024-12-20 12:38:52.662042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 9604 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5\n",
      "2024-12-20 12:38:55.100560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (32, 30, 512)\n",
      "Decoder Output: (32, 16835)\n",
      "Decoder Hidden State: (32, 512)\n",
      "Attention: (32, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE     = 32 # gpu 터져서 줄여서 진행\n",
    "SRC_VOCAB_SIZE = len(kor_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(eng_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024 # gpu 터져서 줄여서 진행\n",
    "embedding_dim = 256 # gpu 터져서 줄여서 진행\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units) \n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer & loss\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # 훈련을 위한 함수를 컴파일\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape: # tf.GradientTape()는 가중치 갱신을 위한 Gradient를 자동 미분으로 계산\n",
    "        enc_out = encoder(src) # Encoder에 소스 문장을 전달하면 컨텍스트 벡터인 enc_out을 리턴\n",
    "        h_dec = enc_out[:, -1] # Encoder의 마지막 time step의 hidden state를 Decoder의 첫번째 hidden state로 사용\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1) # Decoder에 입력으로 전달할 <start> 토큰 생성\n",
    "\n",
    "        for t in range(1, tgt.shape[1]): # 타겟 문장의 길이만큼 반복\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out) # enc_out, hidden state를 기반으로 다음 단어(t시점의 입력)를 예측\n",
    " \n",
    "            loss += loss_function(tgt[:, t], pred) # 타겟 단어와 예측 단어로 손실을 계산\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1) # t 시점의 타겟 문장을 다음 시점의 Decoder 입력으로 사용\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, kor_tokenizer, eng_tokenizer, max_length=30):\n",
    "    sentence = kor_prerocess_sentence(sentence)  # 전처리\n",
    "    mecab = Mecab()\n",
    "    tokenized_sentence = mecab.morphs(sentence)  \n",
    "    tensor = kor_tokenizer.texts_to_sequences([tokenized_sentence])  \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  # 패딩\n",
    "\n",
    "    enc_out = encoder(tensor)\n",
    "    h_dec = enc_out[:, -1]  # Encoder의 마지막 hidden state\n",
    "\n",
    "    # Decoder에 <start> 토큰 입력\n",
    "    dec_src = tf.expand_dims([eng_tokenizer.word_index['<start>']], 1)\n",
    "    result = []  # 번역 결과 저장\n",
    "\n",
    "    for i in range(max_length):\n",
    "        pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "        pred_id = tf.argmax(pred, axis=-1).numpy().flatten()[0]\n",
    "\n",
    "        if eng_tokenizer.index_word[pred_id] == '<end>':\n",
    "            break\n",
    "\n",
    "        result.append(eng_tokenizer.index_word[pred_id])\n",
    "\n",
    "        dec_src = tf.expand_dims([pred_id], 1) # 다음 시점의 Decoder 입력으로 사용\n",
    "\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/498 [00:00<?, ?it/s]2024-12-20 12:39:56.832195: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb5f8abd450 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-12-20 12:39:56.832477: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2024-12-20 12:39:56.832485: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2024-12-20 12:39:56.832491: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2024-12-20 12:39:56.832496: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2024-12-20 12:39:56.837344: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-20 12:39:57.280629: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch  1: 100%|██████████| 498/498 [04:02<00:00,  2.06it/s, Loss 1.4755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Loss: 1.4755\n",
      "\n",
      "=== Epoch 번역 결과 ===\n",
      "소스 문장: 오바마는 대통령이다.\n",
      "번역 문장: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "소스 문장: 시민들은 도시 속에 산다.\n",
      "번역 문장: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "소스 문장: 커피는 필요 없다.\n",
      "번역 문장: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "소스 문장: 일곱 명의 사망자가 발생했다.\n",
      "번역 문장: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 498/498 [01:31<00:00,  5.43it/s, Loss 1.4446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Loss: 1.4446\n",
      "\n",
      "=== Epoch 번역 결과 ===\n",
      "소스 문장: 오바마는 대통령이다.\n",
      "번역 문장: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "소스 문장: 시민들은 도시 속에 산다.\n",
      "번역 문장: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "소스 문장: 커피는 필요 없다.\n",
      "번역 문장: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "소스 문장: 일곱 명의 사망자가 발생했다.\n",
      "번역 문장: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 498/498 [01:31<00:00,  5.44it/s, Loss 1.4447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Loss: 1.4447\n",
      "\n",
      "=== Epoch 번역 결과 ===\n",
      "소스 문장: 오바마는 대통령이다.\n",
      "번역 문장: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "소스 문장: 시민들은 도시 속에 산다.\n",
      "번역 문장: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "소스 문장: 커피는 필요 없다.\n",
      "번역 문장: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "소스 문장: 일곱 명의 사망자가 발생했다.\n",
      "번역 문장: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 498/498 [01:30<00:00,  5.48it/s, Loss 1.4461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Loss: 1.4461\n",
      "\n",
      "=== Epoch 번역 결과 ===\n",
      "소스 문장: 오바마는 대통령이다.\n",
      "번역 문장: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "소스 문장: 시민들은 도시 속에 산다.\n",
      "번역 문장: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "소스 문장: 커피는 필요 없다.\n",
      "번역 문장: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "소스 문장: 일곱 명의 사망자가 발생했다.\n",
      "번역 문장: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 498/498 [01:31<00:00,  5.44it/s, Loss 1.4454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Loss: 1.4454\n",
      "\n",
      "=== Epoch 번역 결과 ===\n",
      "소스 문장: 오바마는 대통령이다.\n",
      "번역 문장: \n",
      "소스 문장: 시민들은 도시 속에 산다.\n",
      "번역 문장: \n",
      "소스 문장: 커피는 필요 없다.\n",
      "번역 문장: \n",
      "소스 문장: 일곱 명의 사망자가 발생했다.\n",
      "번역 문장: \n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6: 100%|██████████| 498/498 [01:31<00:00,  5.45it/s, Loss 1.4460]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Loss: 1.4460\n",
      "\n",
      "=== Epoch 번역 결과 ===\n",
      "소스 문장: 오바마는 대통령이다.\n",
      "번역 문장: \n",
      "소스 문장: 시민들은 도시 속에 산다.\n",
      "번역 문장: \n",
      "소스 문장: 커피는 필요 없다.\n",
      "번역 문장: \n",
      "소스 문장: 일곱 명의 사망자가 발생했다.\n",
      "번역 문장: \n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7: 100%|██████████| 498/498 [01:31<00:00,  5.43it/s, Loss 1.4458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Loss: 1.4458\n",
      "\n",
      "=== Epoch 번역 결과 ===\n",
      "소스 문장: 오바마는 대통령이다.\n",
      "번역 문장: \n",
      "소스 문장: 시민들은 도시 속에 산다.\n",
      "번역 문장: \n",
      "소스 문장: 커피는 필요 없다.\n",
      "번역 문장: \n",
      "소스 문장: 일곱 명의 사망자가 발생했다.\n",
      "번역 문장: \n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8: 100%|██████████| 498/498 [01:31<00:00,  5.43it/s, Loss 1.4456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Loss: 1.4456\n",
      "\n",
      "=== Epoch 번역 결과 ===\n",
      "소스 문장: 오바마는 대통령이다.\n",
      "번역 문장: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "소스 문장: 시민들은 도시 속에 산다.\n",
      "번역 문장: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "소스 문장: 커피는 필요 없다.\n",
      "번역 문장: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "소스 문장: 일곱 명의 사망자가 발생했다.\n",
      "번역 문장: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9: 100%|██████████| 498/498 [01:31<00:00,  5.45it/s, Loss 1.4455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Loss: 1.4455\n",
      "\n",
      "=== Epoch 번역 결과 ===\n",
      "소스 문장: 오바마는 대통령이다.\n",
      "번역 문장: \n",
      "소스 문장: 시민들은 도시 속에 산다.\n",
      "번역 문장: \n",
      "소스 문장: 커피는 필요 없다.\n",
      "번역 문장: \n",
      "소스 문장: 일곱 명의 사망자가 발생했다.\n",
      "번역 문장: \n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 498/498 [01:31<00:00,  5.44it/s, Loss 1.4233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Loss: 1.4233\n",
      "\n",
      "=== Epoch 번역 결과 ===\n",
      "소스 문장: 오바마는 대통령이다.\n",
      "번역 문장: \n",
      "소스 문장: 시민들은 도시 속에 산다.\n",
      "번역 문장: \n",
      "소스 문장: 커피는 필요 없다.\n",
      "번역 문장: \n",
      "소스 문장: 일곱 명의 사망자가 발생했다.\n",
      "번역 문장: \n",
      "=========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # tqdm\n",
    "import random\n",
    "\n",
    "EPOCHS = 10\n",
    "src_list = ['오바마는 대통령이다.', '시민들은 도시 속에 산다.', '커피는 필요 없다.', '일곱 명의 사망자가 발생했다.']\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # 배치 인덱스 셔플\n",
    "    idx_list = list(range(0, kor_tensor.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)  # tqdm으로 진행 상태 표시\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        # 배치 데이터 추출 및 train_step 호출\n",
    "        batch_loss = train_step(\n",
    "            kor_tensor[idx:idx+BATCH_SIZE],\n",
    "            eng_tensor[idx:idx+BATCH_SIZE],\n",
    "            encoder,\n",
    "            decoder,\n",
    "            optimizer,\n",
    "            eng_tokenizer\n",
    "        )\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        # tqdm 상태 업데이트\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))  # Epoch 정보\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))  # 평균 Loss\n",
    "\n",
    "    # Epoch 종료 후 번역 결과 출력\n",
    "    print(f\"\\nEpoch {epoch + 1} Loss: {total_loss / (batch + 1):.4f}\")\n",
    "    print(\"\\n=== Epoch 번역 결과 ===\")\n",
    "    for sentence in src_list:\n",
    "        translation = translate(sentence, encoder, decoder, kor_tokenizer, eng_tokenizer)\n",
    "        print(f\"소스 문장: {sentence}\")\n",
    "        print(f\"번역 문장: {translation}\")\n",
    "    print(\"=========================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회고\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _v1 파일의 경우, 전처리 방식이 길이 40이하로 자른후 토큰화를 진행하여 약 15,000개의 학습 데이터로 진행,  gpu가 계속 터져서 batch_size와 embedding_size를 줄여서 학습을 진행하였지만 성능이 나오지조차 않는..\n",
    "- _v2 파일의 경우, 전치리 방식을 각 언어에 맞게 토큰화 진행후 토큰 길이 40개 이하로 약 65,000개의 학습데이터 진행, 실행 도중 오류로 현재 학습 중, 추후에 업로드 예정이며 이게 현재까지 성능이 그나마 제일 좋아 보인다.\n",
    "- _v3 파일의 경우, 전처리 방식은 _v2와 같고 gpu가 터져서 _v1파일처럼 파라미터를 수정후에 학습을 진행하였지만 결과가 좋지 못하다. 이게 번역기인가?....\n",
    "    내가 썻던 구글 번역기에 대한 안 좋은 기억이 떠오르는거 같다.\n",
    "- 아쉬운 점\n",
    "  - Loung attention을 접목해서 코드 작성하고 싶었는데 코드는 짜봤는데 시간이 없어서 못 돌려봤다, 나중에 따로 업로드 해야겠다.\n",
    "  - attention을 적용한 seq2seq 모델을 배워보고 attention에 대해서 공부할 수 있었다. 수식을 기반으로 이해를 했지만 transformer에도 쓰이는 중요한 메커니즘이니까 주말을 이용해서 Bahdanau_Attention 논문을 정리해봐야겠다. 그래도 최근에 한 프로젝트들이 재밌긴해서 다행이다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
